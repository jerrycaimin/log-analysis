Linux kernal export, and GPFS provide HA for it.

[bring up]
To query the current CNFS configuration, enter:
mmlscluster --cnfs
To temporarily disable CNFS on one or more nodes, enter:
mmchnode --cnfs-disable -N NodeList
To re-enable previously-disabled CNFS member nodes, enter:
mmchnode --cnfs-enable -N NodeList
To permanently remove nodes from the CNFS cluster, enter:
mmchnode --cnfs-interface=DELETE -N NodeList
A sample CNFS configuration
Here is a CNFS configuration example, which assumes the following:
v Your GPFS cluster contains three nodes: fin18, fin19, and fin20
v The host names for NFS serving are: fin18nfs, fin19nfs, and fin20nfs
To define a CNFS cluster made up of these nodes, follow these steps:
1. Add the desired GPFS file systems to /etc/exports on each of the nodes. 2. Create a directory called ha in one of the GPFS file systems by entering:
    mkdir /gpfs/fs1/ha
3. Create a temporary file called /tmp/hanfs-list, which contains the following lines:

fin18 --cnfs-interface=fin18nfs
    fin19 --cnfs-interface=fin19nfs
    fin20 --cnfs-interface=fin20nfs
4. Set the CNFS shared directory by entering: mmchconfig cnfsSharedRoot=/gpfs/fs1/ha
5. Create the CNFS cluster with the mmchnode command, by entering: mmchnode -S /tmp/hanfs-list
6. Access the exported GPFS file systems over NFS. If one or more GPFS nodes fail, the NFS clients should continue uninterrupted.

-----Chapter 28. Implementing a clustered NFS environment on Linux 431

[Failover]
If cnfs failover happens, the failover ip shall include the recovery node ip, this is the way how cnfs failover works.
--->Before failover:
grv-prcnfs06-gpfs, ifconfig--> 10.200.150.32
grv-cnfs01-gpfs, ifconfig--> 10.200.150.221
--->After failover from 01 to 06:
grv-prcnfs06-gpfs, ifconfig--> 10.200.150.32, 10.200.150.221

[Failover Logs]
grv-prcnfs06-gpfs:Mon Sep 24 18:17:46 EDT 2018: mmcommon gpfsrecovernode invoked: myIP=10.200.58.34 recovery and failed nodes="10.200.58.34 10.200.58.21"
grv-prcnfs06-gpfs:Mon Sep 24 18:17:46 EDT 2018: /usr/lpp/mmfs/bin/mmnfsrecovernode invoked
grv-prcnfs06-gpfs:Mon Sep 24 18:17:46 EDT 2018: mmnfsrecovernode: Restarting lockd to start grace
grv-prcnfs06-gpfs:Mon Sep 24 18:17:46 EDT 2018: mmnfsrecovernode: NLM grace period started for recovery of failed nodes 10.200.58.34 10.200.58.34 10.200.58.21
grv-prcnfs06-gpfs:Mon Sep 24 18:17:46 EDT 2018: mmnfsrecovernode: Restarting lockd to start grace

grv-prcnfs03-gpfs:Mon Sep 24 18:17:52 EDT 2018: mmnfsmonitor: NFS clients of node 10.200.58.28 notified to reclaim NLM locks
grv-prcnfs06-gpfs:Mon Sep 24 18:17:52 EDT 2018: mmnfsrecovernode: Initiating IP takeover of 10.200.58.21 due to node failure/recovery
grv-prcnfs03-gpfs:Mon Sep 24 18:17:55 EDT 2018: mmnfsrecovernode: NFS clients of node 10.200.58.28 notified to reclaim NLM locks
grv-prcnfs04-gpfs:Mon Sep 24 18:17:55 EDT 2018: mmnfsrecovernode: NFS clients of node 10.200.58.29 notified to reclaim NLM locks
grv-prcnfs05-gpfs:Mon Sep 24 18:17:55 EDT 2018: mmnfsrecovernode: NFS clients of node 10.200.58.30 notified to reclaim NLM locks
grv-cnfs02-gpfs:Mon Sep 24 18:17:57 EDT 2018: mmnfsrecovernode: NFS clients of node 10.200.58.22 notified to reclaim NLM locks
grv-prcnfs06-gpfs:Mon Sep 24 18:18:00 EDT 2018: mmnfsmonitor: Monitor detected nfsd was not running, will attempt to start it
grv-prcnfs06-gpfs:Mon Sep 24 18:18:00 EDT 2018: mmnfsmonitor: Starting NFS services
grv-prcnfs06-gpfs:Mon Sep 24 18:18:07 EDT 2018: mmnfsmonitor: NFS clients of node 10.200.58.34 notified to reclaim NLM locks
grv-prcnfs06-gpfs:Mon Sep 24 18:18:09 EDT 2018: mmnfsrecovernode: NFS clients of node 10.200.58.21 notified to reclaim NLM locks
grv-prcnfs06-gpfs:Mon Sep 24 18:18:14 EDT 2018: mmnfsrecovernode: NFS clients of node 10.200.58.34 notified to reclaim NLM locks

[Debug]
Step:
1) mmchconfig cnfsDebug=5 -i
2a) Wait on next failover
2b) If it's not easy to recreae failover process, you can try manually:
    Run on grv-cnfs01-gpfs: mmnfstakeover grv-prcnfs06-gpfs
    *This would manually failover grv-cnfs01-gpfs to grv-prcnfs06-gpfs
3) Collect snap and upload again.

[Performance tuning]
关于cNFS性能的问题，可以在模拟环境里做一下测试，并搜集一下数据：
1. 打开GPFS trace
# mmtracectl --set --trace=def -N all
# mmtracectl --start

2. 在server上搜集tcpdump的输出，
# tcpdump -i any -W 10 -C 500 -s 512 -w /bigdir/cnfs.pcap

3. 重现问题

4. 搜集GPFS internal dump
# mmdsh -N all "/usr/lpp/mmfs/bin/mmfsadm dump all > /tmp/mmfs/service.\$(hostname -s).dumpall.\$(date +"%m%d%H%M%S")"

5. 使用Ctrl-C停止tcpdump

6. 停止GPFS trace
# mmtracectl --stop -N all
# mmtracectl --off -N all

7. gpfs.snap

然后把gpfs snap和搜集到的tcpdump发给我们看一下。建议在server上也做一次find，两个结果做个对比。在server上做find的时候需要执行上面的step 1, 3, 4, 6, 7。


[Hang]
cnfsctl activeIp 会调用mmcmi产生adapter上的IP配置信息，然后通过管道传递给awk处理 
mmcmi或awk hang都会最终导致mmfsup不返回


[Failover]
use mmnfsmonitor to monitor and failover:
mmnfsmonitor status
and will update in mmfs.logs

[Switch from ces to nfs]
0. cat /etc/hosts
172.29.92.57 gpfs-cnfs1
172.29.92.58 gpfs-cnfs2
1. mmchnode --ces-disable -N gpfs-serv1,gpfs-serv2
2. enable cnfs:
[root@gpfs-serv1 ~]# cat cnfs_nodes
gpfs-serv1      --cnfs-interface=gpfs-cnfs1
gpfs-serv2      --cnfs-interface=gpfs-cnfs2
[root@gpfs-serv1 ~]# mmchnode -S cnfs_nodes
3. ps -ef| grep nfs
For detail migration steps:
https://www.ibm.com/support/knowledgecenter/en/STXKQY_5.0.2/com.ibm.spectrum.scale.v5r02.doc/bl1adv_ces_migrationcnfstoces.htm
