First response
##################
Hi, I am GPFS L2 and looking at this ticket, I'm now retrieving your snap for detail analysis.
I will bring back update to you once snap is fully reviewed. And the
problem may also be delivered to a dev for deeper review if necessary.

BTW, tomorrow I'll be out of office until Sept 5th. During this time if
it needs urgent help you can put a note in the PMR and one of my
colleagues would handle it accordingly or open a new one.
##################


No gpfs log, tell customer how to get gpfs log:
##################
Hi Robert,

it's Marco here from HPC support.
In order to investigate the issue please collect and upload the GPFS
snap:

1. run /usr/lpp/mmfs/bin/gpfs.snap (on one node it will gather data from
the other nodes)
2. This will generate a file in /tmp/gpfs.snapOut/ called
all.(timestamp).tar please rename it to 02597.077.724.gpfs.tar
3. upload options are available here:
http://www-05.ibm.com/de/support/ecurep/send.html

Your PMR number is 02597,077,724
##################

Query for first analysis
##################
1. Have you noticed this issue previously or is this the first time?
2. What is the current business impact? Is there any loss of access to
data?
3. Have there been any changes recently?
4. A snap on one of the nodes is fine.
##################


Coredump that need reboot:
Sat Sep 1 20:33:35.567 2018: [I] Connected to 10.116.54.214 tcc-rtbd-cogbia02p-v-dat <c0n19>
/usr/lpp/mmfs/bin/runmmfs: line 438: 14980: Memory fault(coredump)
