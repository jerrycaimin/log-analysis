Here is the sample of how to reply "node expel and return" problem:

[Result]
This could be either network glitches or resource contention that caused disk leases renew timeout, which caused the node got expelled.
Suggest administrator check the network environment, or tune the minMissedPingTimeout/maxMissedPingTimeout to cover up network glitches and makes gpfs more tolerante to network issues.

[Analysis]
We can see from mmfs log that
1st time node rstnsd2 expel, in rstnsd1 mmfs log:
Mon Nov 13 14:12:56.015 2017: [E] Node 130.145.134.249 (rstnsd2-clus.natlab.research.philips.com) is being expelled because of an expired lease. Pings sent: 60. Replies received: 60.

In rstnsd2 mmfs log:
Mon Nov 13 14:13:31.558 2017: [N] Disk lease period expired 0.010 seconds ago in cluster rst1.natlab.research.philips.com. Attempting to reacquire the lease.
after a while:
Mon Nov 13 14:15:16.715 2017: [N] Disk lease reacquired in cluster rst1.natlab.research.philips.com.
rstnsd2 seems recovered automatically.


2nd time node rstnsd2 expel, in rstnsd1 mmfs log:
Tue Nov 14 10:31:18.256 2017: [E] Node 130.145.134.249 (rstnsd2-clus.natlab.research.philips.com) is being expelled because of an expired lease. Pings sent: 60. Replies received: 60.

In rstnsd2 mmfs log:
Tue Nov 14 10:31:37.297 2017: [N] Disk lease period expired 0.020 seconds ago in cluster rst1.natlab.research.philips.com. Attempting to reacquire the lease.
after a while:
Tue Nov 14 10:35:09.652 2017: [N] Disk lease reacquired in cluster rst1.natlab.research.philips.com.
rstnsd2 seems recovered automatically, too.

from the internaldump "dump tscomm" section it also showed that:
rstnsd1:
TCP Connections between nodes:
        node destination     status     err b  sock      sent     recvd   sseq   rseq  retry   ver  ostype
      <c0n1> 130.145.134.249 broken     233 -    -1  60919405  60919108      1  65535      0  1603  Linux/B
      <c0n2> 130.145.135.233 connected    0 -    62  17422715  17457817  14467  25297      0  1603  Linux/B
      <c0n3> 130.145.135.232 connected    0 -    72 556595505 557830046  30060  58886      0  1704  Linux/L
      <c0n4> 130.145.135.234 connected    0 -   179 403676223 404574230   5964  26147      0  1704  Linux/L
      <c0n5> 130.145.134.243 connected    0 -    86  96792068  96989097  19976  62369      0  1512  Linux/L
      <c0n6> 130.145.135.215 connected    0 -    97  91566818  91761929  37632  12649      0  1512  Linux/L
      <c0n7> 130.145.133.248 connected    0 -   107 135627006 135831097  58294  42373      0  1512  Linux/L
	...... <All connected besides rstnsd2>

rstnsd2:
TCP Connections between nodes:
        node destination     status     err b  sock      sent     recvd   sseq   rseq  retry   ver  ostype
      <c0n0> 130.145.135.231 broken     233 -    -1  60919108  60919405      1  65535      0  1603  Linux/B
      <c0n2> 130.145.135.233 broken     233 -    -1   1334633   1337715      1  65535      0  1603  Linux/B
      <c0n3> 130.145.135.232 broken     233 -    -1   9999432  10024920      1  65535      0  1704  Linux/L
      <c0n4> 130.145.135.234 broken     233 -    -1  16252301  16284556      1  65535      0  1704  Linux/L
      <c0n5> 130.145.134.243 broken     233 -    -1   1329936   1333013      1  65535      0  1512  Linux/L
	...... <All broken>

It seems rstnsd2 got network issue when problem happened, and after a while it returned, which we can consider this as network glitches caused the problem

[Background]
Here is the gpfs node expel mechanism:

There are two types of node expels:
1. Disk Lease Expiration - GPFS uses a mechanism referred to as a disk
lease to prevent file system data corruption by a failing node. A disk
lease grants a node the right to submit IO to a file system. File system
disk leases are managed by the Cluster Manager of the file system's home
cluster. A node must periodically renew it's disk lease with the Cluster
Manager to maintain it's right to submit IO to the file system. When a
node fails to renew a disk lease with the Cluster Manager, the Cluster
Manager marks the node as failed, revokes the node's right to submit IO
to the file system, expels the node from the cluster, and initiates
recovery processing for the failed node.

2. Node Expel Request - GPFS uses a mechanism referred to as a node
expel request to prevent file system resource deadlocks. Nodes in the
cluster require reliable communication amongst themselves to coordinate
sharing of file system resources. If a node fails while owning a file
system resource, a deadlock may ensue. If a node in the cluster detects
that another node owing a shared file system resource may have failed,
the node will send a message to the file system Cluster Manger
requesting the failed node to be expelled from the cluster to prevent a
shared file system resource deadlock. When the Cluster Manager receives
a node expel request, it determines which of the two nodes should be
expelled from the cluster and takes similar action as described for the
Disk Lease expiration.

Both types of node expels, Disk Lease Expiration and Node Expel Request,
will result in a node unmounting the GPFS file system and possible job
failure. Both type of expels are often a result of some type of network
issue.

[Action Plan]
1. Adminitrator check potential network glitches problem.
2. Tune the GPFS config to cover up network glitches and makes gpfs more tolerante to network issues:
	a) Tune minMissedPingTimeout to 60 and maxMissedPingTimeout to 120.
	Changing minMissedPingTimeout from 3s to 60s to cover over short
	network glitches. It allows the lease to be overdue by one minutes
	before a node is declared dead.
	This value only needs to be changed on quorum nodes, since GPFS daemon
	needs to be restarted to take effect the new value, you can reboot
	quorum nodes one at a time leaving current cluster manager for last this
	way cluster stays up all the time.
	mmchconfig minMissedPingTimeout=60
	mmchconfig maxMissedPingTimeout=120
	It's better to recycling daemon on cluster manager, select another quorum as new
	cluster manager (via mmchmgr -c $nodename).

	b) Tuning "failureDetectionTime", interval of disk lease request. So increasing this config would
	increase the lease duration thus giving more time between marking a node
	as failed when it doesn't renew its lease fast. Default is 35s.

	Simply here is the logic where the failed node was not the cluster
	manager:

	1) We see the last time the failed node renewed its lease
	2) The cluster manager detects that the lease has expired (after
	failureDetectionTime), and starts pinging the node.
	3) The cluster manager decides that the node is dead and runs the node
	failure protocol
	4) The file system manager starts log recovery

	For detail of each config, please refer to here:
	https://www.ibm.com/support/knowledgecenter/en/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1adv_optional.htm

[Below explainations are for failureDetectionTime and leaseRecoveryWait]
The leaseRecoveryWait parameter defines how long the FS manager of a
filesystem will wait after the last known lease expiration of any failed
nodes before running recovery. A failed node cannot reconnect to the
cluster before recovery is finished. The default of leaseRecoveryWait
value is 35 seconds. Making this value smaller increases the risk that
there may be IO in flight from the failing node to the disk/controller
when recovery starts running. This may result in out of order IOs
between the FS manager and the dying node.

From above statements, you know leaseRecoveryWait is not part of
failureDetectionTime.

In most cases where a node is expelled from the cluster there is a
either a problem with the network or the node running out of resources
like paging. The failureDetectionTime does not protect against network
glitches, it just changes the frequency of acquiring a lease.
The leaseRecoveryWait controls how long the FS mgr has to wait before
running recovery but once you set minMissedPingTimeout longer than
leaseRecveryWait it doesn't matter.

Current minMissedPingTimeout is 3 seconds, suggest to increase it to 60
seconds.

For example, mmchconfig minMissedTimeout=60
Changing MissedPing times only has to be done on the quorum nodes,
because only the cluster manager uses them to determine how long to wait
before kicking a bad node out. Recycle quorum nodes one at a time
leaving current cluster manager for last, use mmchmgr -c to force
cluster manager to another quorum nodes,this way cluster stays up all
the time.

[Below explainations are for failureDetectionTime and leaseRecoveryWait]
1) failureDetectionTime = interval of Disk lease request, default 35s.
2) leaseRecoveryWait = After disk lease requested, how much time will cluster manager wait and determine if node is dead or alive. Default is 35s. 
Note: If leaseRecoveryWait < minMissedPingTimeout, leaseRecoveryWait = minMissedPingTimeout
3) minMissedPingTimeout = min of ping timeout
Tip: leaseRecoveryWait is not recommended to set < 35s, as in overload situation, it will increase time of disk lease response, less than 35s
may harm stability or lead data inconsistency.

[Suggestion from senior member]
provided an AP with 9 recommendations to optimize the network
performance:

According to the mmlscluster output, the 192.1.213.1xxx network is used
for GPFS daemon traffic. While most nodes use bond1 as GPFS interface,
there are three nodes (pur02, pur03, pur04) use eth3 alone. Not sure if
this is intended or not, please double check.

2. Although the other nodes use bond1, but the mode is set to
active-backup, so actually, we can only achieve 10G bandwidth from the
bonded interface. We recommend with bond mode 4 (802.3ad, require
support from switch), and xmit_hash_policy layer3+4.
Please refer to:
https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectru
m.scale.v4r23.doc/bl1ins_aggregate.htm

3. The Ring parameters for the interface is set quite low comparing to
its pre-set maximum. Please contact your NIC vendor to see if those
should be bump up to the maximum. Note that, the settings using ethtool
won't survive reboot, you can add the change into your boot scripts.

4. High rate of reordering packets. Reordering can hurt the TCP
performance. You might want to involve your network team to check this.

5. Nodes in pur are not enabled with net.ipv4.tcp_sack and
net.ipv4.timestamp, while the nodes in other clusters are. With a
network of reordered packets, ack loss or packet replication, you can
try enable sack and timestamp which can help..

Please refer to the below link:
https://www.ibm.com/developerworks/community/wikis/home?lang=en#!/wiki/G
eneral%20Parallel%20File%20System%20(GPFS)/page/Linux%20Performance

7. The pagepool 1G and maxFilesToCache 4000 is too small for your
applications.. In the 3s of trace when the node was expelled, I can see
your applications performed lookup against 2018 files/directorys. And
also 2% dirty steal for the file cache...... With 120G physical memory,
and most of them free, I would recommend to try increase the pagepool to
 4G ~ 8G, and increase maxFilesToCache, eg. 10K . This can help to
decreasing the NSD data written/read data to/from the NSD server over
network.....

Please refer to more details of those parameters in our wiki as below.
https://www.ibm.com/developerworks/community/wikis/home?lang=en#!/wiki/G
eneral%20Parallel%20File%20System%20(GPFS)/page/Tuning%20Parameters

8. vm.min_free_kbytes is too small as well. Please increase them to
5%~6% of your physical memory but no more than 2G.
https://www.ibm.com/support/knowledgecenter/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1ins_suse.htm

9. In your ESS cluster, I noticed verbsRdma is enabled, but both RDMA
ports were disabled... Not sure if you plan to use RDMA or not, you
might want to involve your network team to have a check.


Customer added following options:

# added in /etc/sysctl.conf
net.ipv4.tcp_sack = 1
net.ipv4.tcp_timestamps = 1
vm.min_free_kbytes = 2000000
