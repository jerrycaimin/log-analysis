From GPFS perspective, there is no specific criteria that how fast it's good, it depends on what type of disk you have(hdd or ssd), how the backend storage connect to nsd server(fc or ethernet), and which protocal you use(san or scsi)
We can see from an ESS env dd write speed raised to 4.8G/s, and a local x86 vm env dd write is only 150M/s, both of them can be normal. 

1. Test write speed:
	dd if=/dev/zero bs=1M count=1000 of=/gpfs/gpfs0/test2
2. Test read speed:
	dd if=/gpfs/gpfs0/test2 bs=1M count=1000 of=/dev/null

Then from L2 side, we check the long waiters of "I/O completion on disk dm-{x}", it's environment independent, and if we see it's larger than 100ms, disk problem will be the most possible cause.
From the uploaded dumpall log I can found a lot of long waiters:
Waiting 0.7894 sec since 09:46:57, monitored, thread 26929 CleanBufferThread: for I/O completion on disk dm-0
Waiting 0.7842 sec since 09:46:57, monitored, thread 26926 CleanBufferThread: for I/O completion on disk dm-3
Waiting 0.7806 sec since 09:46:57, monitored, thread 26487 WritebehindWorkerThread: for I/O completion on disk dm-2
...

The waiting sec is 700ms that beyond 100ms, it's not good and high possible that some disk has io issue need to check.

Also I generate the report with performance tool on dumpsome file, found this:
>iohist.awk slowdata=1000 internaldump.* | awk {'print $9'} | sort 

                         Num of    Time in   NSD        FS                                                                      
Time     R/W   Buf type sectors    seconds  name      name     K/sec  type      NSD server   Context                   Thread   
-------- ---   -------- -------  --------- ----- --------- ---------  ----  -------------- --------- ------------------------   
09:45:43   W       data    2048  0.7746420 data1   gpfs001   1321.90  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7841620 data1   gpfs001   1305.85  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7795770 data3   gpfs001   1313.53  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7807510 data1   gpfs001   1311.56  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7823030 data6   gpfs001   1308.96  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7968570 data5   gpfs001   1285.05  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7902930 data6   gpfs001   1295.72  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7903240 data2   gpfs001   1295.67  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.8008930 data1   gpfs001   1278.57  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.8040610 data4   gpfs001   1273.54  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.8008990 data4   gpfs001   1278.56  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7906390 data1   gpfs001   1295.15  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7925760 data2   gpfs001   1291.99  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.8006300 data4   gpfs001   1278.99  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.8013960 data2   gpfs001   1277.77  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.8002510 data1   gpfs001   1279.60  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7926290 data4   gpfs001   1291.90  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7881690 data1   gpfs001   1299.21  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7949620 data5   gpfs001   1288.11  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7957370 data5   gpfs001   1286.86  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7974250 data6   gpfs001   1284.13  lcl                    Cleaner CleanBufferThread          
                         Num of    Time in   NSD        FS                                                                      
Time     R/W   Buf type sectors    seconds  name      name     K/sec  type      NSD server   Context                   Thread   
-------- ---   -------- -------  --------- ----- --------- ---------  ----  -------------- --------- ------------------------   
09:45:43   W       data    2048  0.8014650 data1   gpfs001   1277.66  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7990050 data3   gpfs001   1281.59  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7856300 data4   gpfs001   1303.41  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7965130 data1   gpfs001   1285.60  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7911060 data6   gpfs001   1294.39  lcl                    Cleaner CleanBufferThread          
09:45:43   W       data    2048  0.7806260 data6   gpfs001   1311.77  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7825000 data3   gpfs001   1308.63  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7944060 data4   gpfs001   1289.01  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7945820 data5   gpfs001   1288.73  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7889710 data6   gpfs001   1297.89  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7780630 data2   gpfs001   1316.09  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7812380 data3   gpfs001   1310.74  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7818320 data5   gpfs001   1309.74  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7762480 data6   gpfs001   1319.17  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7852490 data1   gpfs001   1304.04  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7831570 data1   gpfs001   1307.53  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7836300 data3   gpfs001   1306.74  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7869150 data3   gpfs001   1301.28  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7890260 data5   gpfs001   1297.80  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7867940 data1   gpfs001   1301.48  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7869900 data2   gpfs001   1301.16  lcl                   Prefetch WritebehindWorkerThread    
                         Num of    Time in   NSD        FS                                                                      
Time     R/W   Buf type sectors    seconds  name      name     K/sec  type      NSD server   Context                   Thread   
-------- ---   -------- -------  --------- ----- --------- ---------  ----  -------------- --------- ------------------------   
09:45:43   W       data    2048  0.7913640 data3   gpfs001   1293.97  lcl                   Prefetch WritebehindWorkerThread    
09:45:43   W       data    2048  0.7979510 data4   gpfs001   1283.29  lcl                   Prefetch WritebehindWorkerThread    


This shows the IO situation of 9:45:43, on multiple nsd data1-6 of fs gpfs001, each of them write speed are larger than 1000K/s, from gpfs perspective in this point it's acceptable and only need to check when bussiness got impact.

[Action Plan]
Could customer answer the following question and try to do:
1. How much space does gpfs001 left? use "mmdf gpfs001" could see the result, if less capacity left, it will impact the performance extremely.
2. Does the performance behaviour like this all the time since gpfs installed, or just happen on this gpfs001 fs(created on 17-11-17)?
3. Is there any bussiness impact on specified case?
4. Suggest admin check each disk, check if any single disk wrong? Ex, try dd on each single disk, or use iostat to check any disk abnormal during dd a large file, to narrow down the io problem.
5. Try different blocksizes, here is the sample table for speed to each type of blocksize:
	512K = 180 MB/s
	1 M = 200 MB/s
	2 M = 220 MB/s
	4 M = 1GB/s
	8 M = 1,1 GB/s
	16M = 1,2 GB/s
	(Solved for pmr 20848,999,724)
6. use tool in /usr/lpp/mmfs/samples/net/nsdperf.C.
	a) Build follow as /usr/lpp/mmfs/samples/net/README
	b) Run built out file.
	c) set client and server and run.
	
	
	
	
From client side, I take one data write of 1MB as an example. The client sends nsdMsgWriteExt via TCP to ESS. This message contains start, len, DA address etc of the message to be wrote. The whole message took 0.168925s which is quite slow:

  96.354742 111021 TRACE_TS: tscSend: service 00100001 msg 'nsdMsgWriteExt' n_dest 1 data_len 104 msg_id 931061 msg 0x7F807002D200 mr 0x7F807002D020 <--- begin send NSD write request
  96.523667 111291 TRACE_TS: service_message: enter: msg 'reply', msg_id 931061 seq 10364 ackseq 10411, from <c1n6> 10.1.0.230, active 0  <--- NSD data has been wrote into ESS IO node

So let's go to server side, and see what happend:

  39.368965  33184 TRACE_TS: tscHandleMsgDirectly: service 00100001, msg 'nsdMsgWriteExt', msg_id 931061, len 104, from <c0n152> 10.1.33.113 <--- get the NSD write request
  39.368967  33184 TRACE_NSD: queueMsgReadWriteFast: trying queue GNR type NsdQueueGNR [8]
  39.368969  33184 TRACE_NSD: mutexAcquire NsdQueueGNR[8] NsdQueueUseNsdIO 0x3fff8c4470e0
  39.368970  33184 TRACE_NSD: initNsdReqReceiver: queueP 0x3FFF8C4470C0 nsdReqP 0x3FFE04239140 rctxP 0x3FFE30019368   <--- extract NSD write request from nsdMsgWriteExt, put it into a queue, and another thread will use RDMA to get the data from client
  39.368972  33184 TRACE_NSD: mutexRelease NsdQueueGNR[8] NsdQueueUseNsdIO 0x3fff8c4470e0
  39.368973  33184 TRACE_MUTEX: Signaling fast condvar for signal 0x3FFB30000F80 NSDQFastCondvar
  39.368976  33184 TRACE_NSD: queueMsgReadWriteFast: exit: err 306
  39.368977  33184 TRACE_NSD: nsdMsgHandler exit: err 306
  39.368980  33184 TRACE_BASIC: cxiRecv: sock 378 buf 0x3FFDAC011D38 len 1024 flags 0 failed with err 11
  39.368981  33184 TRACE_TS: receiverEvent exit: sock 378 err 54 newTypes 1 state reading header
  39.368982  30079 TRACE_NSD: nsdMsgReadWriteExtFinder: extensionLen 12, receivedFlags 0x80000023
  39.368983  30079 TRACE_NSD: nsdMsgReadWriteExtFinder: found i 0, bit 0x80000000, len 8
  39.368985  30079 TRACE_NSD: initNsdReqWorker: NSD rdwr disknum 4 sector 16920475648 nbytes 1048576 tag1 131134 tag2 3 ck NsdCksum_Ck64
  39.368988  30079 TRACE_NSD: mutexAcquire NsdQueueGNR[8] NsdQueueUseNsdIO 0x3fff8c4470e0
  39.368991  30079 TRACE_NSD: mutexRelease NsdQueueGNR[8] NsdQueueUseNsdIO 0x3fff8c4470e0
  39.368993  30079 TRACE_NSD: processRequest (0x3FFE04239140) enter: nsdId 00E60A01:592BE23B da 4:16920475648 nBytes 1048576 rdwrFlags 0x100002 ck NsdCksum_Ck64 queue GNR type NsdQueueGNR [8]
       <---- the thread begin to pull data from NSD client to NSD server
  39.368996  30079 TRACE_VDISK: vhLookup: vdisk ess06_da1_fs01_d43 id 00E60A01:592BE23B vtrack 4130975 bucket 0xB91F flags 0x0
  39.368998  30079 TRACE_VDISK: vhLookup: vdisk ess06_da1_fs01_d43 id 00E60A01:592BE23B vtrack 4130975 list cleanPartial flags 0x0 found 0x1E00286576E0 hold 1 attempts 1
  39.369022  30079 TRACE_VDISK: getWriteIovec: nBytes 1048576 numVBuffs 144
  39.369029  30079 TRACE_RDMA: verbsServer:   enter WR <c0n152> tag 131134 3 nBytes 1048576 nRmr 1 nIov 132
       <--- begin RDMA read
  39.369039  30079 TRACE_RDMA: verbsServer:   rdma WR rweP 0x3FFC18004CB0 nBytes 1048576 reterr 0 currLen 794624 nSge 28 cP 0x30001740000 sP 0x3B364064000
  39.369042  30079 TRACE_MUTEX: Waiting on fast condvar for signal 0x3FFC18004CC0 RdmaRead_Server
  39.369671  33155 TRACE_RDMA: verbsDtoThread: add deviceCqP 0x3FFA37292558 cqP 0x3FFEC400A000 nEvents 118
  39.369673  33155 TRACE_RDMA: verbsDtoThread: event WR rweP 0x3FFC18004CB0 cnt 794624 tag 131134 3 len 794624
  39.369673  33155 TRACE_MUTEX: Signaling fast condvar for signal 0x3FFC18004CC0 RdmaRead_Server
  39.369680  30079 TRACE_RDMA: verbsServer:   rdma WR rweP 0x3FFC18004CB0 nBytes 1048576 reterr 794624 currLen 253952 nSge 9 cP 0x30001802000 sP 0x2335E84E000
  39.369682  30079 TRACE_MUTEX: Waiting on fast condvar for signal 0x3FFC18004CC0 RdmaRead_Server
  39.369890  33155 TRACE_RDMA: verbsDtoThread: add deviceCqP 0x3FFA37292558 cqP 0x3FFEC400A000 nEvents 119
  39.369892  33155 TRACE_RDMA: verbsDtoThread: event WR rweP 0x3FFC18004CB0 cnt 253952 tag 131134 3 len 253952
  39.369893  33155 TRACE_MUTEX: Signaling fast condvar for signal 0x3FFC18004CC0 RdmaRead_Server
  39.369897  30079 TRACE_RDMA: verbsServer:    exit WR <c0n152> tag 131134 3 reterr 1048576
       <--- All the 1MB data read from NSD client side completes. The total time spent for RDMA transfer is less than 0.000932 seconds, very fast. Now data to be wrote has arrived at NSD server side.
                    
  39.369898  30079 TRACE_VDISK: QIO: mediumWrite data tag 131134 3 vdiskName ess06_da1_fs01_d43 startSec 16920475648 nSectors 2048 by (NSDThread)
      <--- NSD server begin to put the IO request into queue.

   <TODO: needs more analysis between QIO and FIO>                      
  39.537600  30079 TRACE_VDISK: FIO: mediumWrite data tag 131134 3 vdiskName ess06_da1_fs01_d43 startSec 16920475648 nSectors 2048 err 0
      <--- Finished IO
  39.537603  30079 TRACE_VDISK: postIO: mediumWrite data tag 131134 3 vdiskName ess06_da1_fs01_d43 startSec 16920475648 nSectors 2048 err 0 duration 0.167702000
      <--- IO duration is 0.1677020 seconds. So almost all IO time is spent on ESS IO write.
  39.537609  30079 TRACE_TS: tscSendReply: service 00100001, msg 'nsdMsgWriteExt', msg_id 931061, replyLen 0
      <--- ESS IO completes. Needs to write a reply to client, telling it the IO completes
     
     
#####check output#########
$ for i in */mmfsadm*
> do
> sys=$(echo $i| cut -f 1 -d '_')
> iohist=$(dumpsome $i | grep iohist:)
> echo "$sys: $iohist"
> done 